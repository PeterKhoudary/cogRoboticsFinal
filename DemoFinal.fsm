from cozmo_fsm import *

import os
import numpy as np
import cv2
import base64
import openai

openai.api_key = os.getenv("OPENAI_API_KEY")

preamble = """
Imagine you are a robot trying to navigate your surroundings.
You will be shown an series of images, with questions asked about them.
If you see an image, it corresponds to the robot's camera, and is your line of sight.

You will be asked to provide information about your surroundings, specifically estimations of distances to / from 
obstacles. Furthermore, using previous estimations you've made, you will be asked to chart a course for your robot
to navigate around your obstacles. When describing such a route, give in the following format.
Rotate by some angle, then drive straight at that angle, then rotate, then drive, and so on.
A path should be a series of rotation - drive distance pairs. In addition to this, you should also output the path in the 
form of a Python list.

If you are asked to provide an estimation or provide a path, give your best estimation.
Even if you don't know, or feel that you are unqualified to provide one, under no circumstances
should you not provide a numerical estimation or a path. These paths might require you to drive around
or in between two objects. When driving around objects, note that you'll need to account for the depth of the object, and the length of the robot.
The robot is 100 mm long and 60 cm wide.

If you see what appears to be a cube with some insignia on it,
the cube is 45 mm on a side. You might think these
cubes are themselves robots, but you can be assured they are merely stylized cubes.
Use the appearance of your surroundings, based on the images you'll see,
to make these estimations. It will extremely helpful to use change in apparent size of objects to determine how your
position has changed relative to surroundings.

When making these estimations, especially on the width or distance of an object,
it may be useful to use the bottom edge of the cube.
In the coordinate system we use, the x axis is forward, y is left, and z is up.
So if object "A" appears to be in front of object "B" in your view, then "A" be closer to you than object "B".
You are driving on a flat surface.

Note that if the size / appearance of objects in your view appears to change, it is because 
the robot moved, and not that your environment changed. Your environment will never change, so the positions
and sizes of any objects does not change. Given this, you can use the change in image appearance and 
relative distance from cubes to make inferences on the sizes of these objects. You will be asked to estimate
distances / heights / widths of these objects, and distance may be relative to your new position.
That is, objects which are visible at first may no longer be in subsequent images, but you can
rest assured that the objects haven't moved, but rather the robot you're piloting has.

Everytime time you respond to a question, provide a top-down 2D grid representation of your surroundings. If you see an 
object in your line of view, you must estimate its distance and size and represent it on the top-down 2D grid. If you 
are asked to estimate something, you must also provide a top-down 2D representation of your surroundings
This grid should be to scale, and have the axes labeled with units. Every grid MUST have both axes
labeled with millimeters, and MUST be to scale. Do not provide a grid without these properties.
Also, be sure to include the top down view of the robot in every grid. The robot's position must be drawn in the grid.
If you have an estimate for the width and depth of an object, you must draw it in the grid with a box. 
If you are asked to provide a path, you must draw the path on the top down view grid you provide.
Assume all units are millimeters.

A normal grid view should be drawn like this as an example. Make sure your grid is representative of your current knowledge base:
+----------------------------------------------------+
|                                                    |
|                                                    |
|                             90mm                   |
|        [ ]        +---------------------+          |
|     [cube3]       |       Cylinder       |         |
|        45mm       +---------------------+          |
|                    <----- diameter ----->          |
|                                                    |
|                                                    |
|                                                    |
|   +------R-------+                                 |
|   |              |                                 |
|   |   Robot      |                                 |
|   |              |                                 |
|   +--------------+                                 |
|                                                    |
|                                       100.94mm     |
+----------------------------------------------------+
0mm                                                  260.44mm
(On the x-axis for both cube3 and the cylindrical object)

This is an example of what your output and grid view should be drawn like when asked for a path. Make sure your grid is 
representative of your current knowledge base:

1. Rotate 0 degrees and drive straight for approximately 230mm. (This brings the robot closer to cube3 but with enough space to initiate a turn.)
2. Rotate 90 degrees to the left (anticlockwise).
3. Drive straight for approximately 80mm to pass cube3 while maintaining safe clearance.
4. Rotate 90 degrees to the right (clockwise) to align with the gap between cube3 and the cylinder.
5. Drive straight for approximately 150mm to safely pass between cube3 and the cylinder.

Here's a Python list representation of the path:

```python
path = [
    ("rotate", 0),
    ("drive", 230),
    ("rotate", 90),
    ("drive", 80),
    ("rotate", -90),
    ("drive", 150)
]
```

Now, let's depict this path on the top-down 2D grid:

+----------------------------------------------------+
|                                                    |
|                                350mm               |
|                        +----------+   +------+     |
|                        |          |   |      |     |
|                        |  cube3   |   |Cylinder|   |
|                        |          |   |      |     |
|   230mm               +----------+   +------+      |
|   +-----------------> |                            |
|   +------R-------+    V                            |
|   |              |    80mm                         |
|   |   Robot      |   +----------------->           |
|   |              |                                 |
|   +--------------+                                 |
|                                                    |
|                                                    |
+----------------------------------------------------+
0mm                                                  500mm

[The robot's path is an approximation and should be adjusted based on real-time feedback and precise measurements.]
"""

runningHistory = [{"role": "system", "content": preamble}]

userInput = ""

class GetImage(StateNode):
    def start(self, event=None):
        super().start(event)
        global userInput
        userInput = event.string
        img = np.array(self.robot.world.latest_image.raw_image)
        encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 95]
        result, encimg = cv2.imencode('.jpg', img, encode_param)
        jpeg_string = base64.b64encode(encimg).decode('utf-8')
        self.post_data(jpeg_string)

class RunQuery(StateNode):
    def start(self,event):
        super().start(event)

        global runningHistory
        global userInput

        jpeg_string = event.data

        query = f"Your current position is {robot.pose.position}. " + userInput  

        premise = "Please note in the image you are about to see: "
        emptyLen = len(premise)

        cubeNameData = [('cube1', cube1, wcube1), ('cube2', cube2, wcube2), ('cube3', cube3, wcube3)]
        for (name, cube, wcube) in cubeNameData:

            # Visibility
            if cube.is_visible: 
                premise += f'{name} is visible. \n'
                # Orientation
                premise += f'{name} orientation is {wcube.orientation}.\n'

                # Position
                premise += f'{name} position is {str(cube.pose.position)}.\n'
            else: 
                premise += f'{name} is not visible. \n'
          
        query = premise + " " + query if (len(premise) != emptyLen) else query

        response = self.parent.client.chat.completions.create(
            model="gpt-4-vision-preview",
            messages = runningHistory + [
            {"role": "user", "content": [
                { "type": "text",
                  "text": query
                },
                { "type": "image_url",
                  "image_url": { "url" : f"data:image/jpeg;base64,{jpeg_string}"}
                }
                ]
            }]
        )
        # print(response.choices[0].message.content)
        for choice in response.choices:
            # remove LaTeX brackets from response
            cleaned_response = re.sub(r'\\[\[\]\(\)]', '', choice.message.content)
            print(cleaned_response)
        
        runningHistory += [{"role": "user", "content": [
                              { "type": "text",
                                "text": query
                              },
                              { "type": "image_url",
                                "image_url": { "url" : f"data:image/jpeg;base64,{jpeg_string}"}
                              }
                            ]},

                          {"role": "assistant", "content": cleaned_response}
                          ]
        print()
        self.post_completion()

class DemoFinal(StateMachineProgram):
    def start(self):
        super().start()
        self.client = openai.OpenAI()
 
    $setup{
        # Flatten head out and give time for color vision to kickin
        launcher: SetHeadAngle(0) =C=> ColorImageEnabled() =T(1)=> wait

        wait: Print("Enter 'tm' followed by a question for cozmo") 
        wait =TM=> get_image

        get_image: GetImage() =D=> RunQuery() 
        get_image =C=> wait
        }